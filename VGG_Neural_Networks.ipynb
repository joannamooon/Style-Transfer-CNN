{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Notes written from: https://thegrigorian.medium.com/understanding-vgg-neural-networks-architecture-and-implementation-400d99a9e9ba\n",
        "# Understanding VGG Neural Networks\n",
        "- VGG (Visual Geometry Group) is a CNN proposed by researchers from Oxford in 2014.\n",
        "- Main challenge in image recognition is capturing intricate features and patterns in images.\n",
        "  - primary motivations for **deep architectures** was the realization that visual data contains features at various levels of abstraction\n",
        "- VGG tries to solve this problem with depth - by stacking many layers, VGG created a hierarchy of features that allowed it to grasp the essence of images\n",
        "- Shallower architectures like LeNet and AlexNet captures simpler features, and encounters bottlenecks when confronted with complex images\n",
        "\n",
        "## Explaining VGG\n",
        "- The primary focus of the VGG architecture is on increasing the depth of the network while using simple and uniform convolutional layers.\n",
        "- have various VGG architectures like VGG13, VGG19 -> number = total layers\n",
        "\n",
        "1. Convolutional Layers\n",
        "  - use 3 x 3 filters\n",
        "  - allows the network to capture a wide range of features at different scales, enabling richer representations\n",
        "2. Pooling Layers\n",
        "  - After every two to three convolutional layers, max-pooling layers are used for spatial down-sampling\n",
        "  - Max-pooling helps in reducing the spatial dimensions while retaining important features\n",
        "3. Fully-Connected Layers\n",
        "  - consolidate the learned features and produce class predictions\n",
        "\n",
        "- use ReLU as activation functions\n",
        "- networks are trained using stochastic gradient descent with momentum\n",
        "- dropout is used to reduce overfitting\n",
        "\n",
        "## VGG Blocks\n",
        "- blocks of convolutional and pooling layers\n",
        "- stack these blocks to discern high-level features that define the identity of objects in images\n",
        "\n"
      ],
      "metadata": {
        "id": "dw7IWYr3-FZQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation using TensorFlow"
      ],
      "metadata": {
        "id": "hyBDWGvEAxqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "0ERKofWWA8su"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yCQbbLe-Aib"
      },
      "outputs": [],
      "source": [
        "class Block(keras.Model):\n",
        "  def __init__(self, filters, kernel_size, repetitions, strides=2, pool_size=2):\n",
        "    super(Block, self).__init__()\n",
        "    self.filters = filters\n",
        "    self.kernel_size = kernel_size\n",
        "    self.repetitions = repetitions\n",
        "\n",
        "    # create a convolutional layer repetition times\n",
        "    for i in range(repetitions):\n",
        "      vars(self)[f'conv2d_{i}'] = keras.layers.Conv2D(filters, kernel_size, activation='relu', padding='same')\n",
        "\n",
        "    # create max pooling layer after the convolutional layers\n",
        "    self.max_pool = keras.layers.MaxPool2D(pool_size=pool_size, strides=strides)\n",
        "\n",
        "  # takes x (input) and processes it through the block\n",
        "  def call(self, x):\n",
        "    conv2D_0 = vars(self)['conv2d_0']\n",
        "    x = conv2D_0(x)\n",
        "\n",
        "    for i in range(1, self.repetitions):\n",
        "      conv2D_i = vars(self)[f'conv2d_{i}']\n",
        "      x = conv2D_i(x)\n",
        "\n",
        "    max_pool = self.max_pool(x)\n",
        "    return max_pool\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyVGG(keras.Model):\n",
        "  def __init__(self, input_shape, num_classes=10):\n",
        "    super(MyVGG, self).__init__()\n",
        "    self.block_a = Block(filters=64, kernel_size=3, repetitions=2)\n",
        "    self.block_b = Block(filters=128, kernel_size=3, repetitions=2)\n",
        "    self.block_c = Block(filters=256, kernel_size=3, repetitions=3)\n",
        "    self.block_d = Block(filters=512, kernel_size=3, repetitions=3)\n",
        "    self.block_e = Block(filters=512, kernel_size=3, repetitions=3)\n",
        "    self.flatten = keras.layers.Flatten()\n",
        "    self.dense_1 = keras.layers.Dense(256, activation='relu')\n",
        "    self.dense_2 = keras.layers.Dense(num_classes, activation='softmax')\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.block_a(inputs)\n",
        "    x = self.block_b(x)\n",
        "    x = self.block_c(x)\n",
        "    x = self.block_d(x)\n",
        "    x = self.block_e(x)\n",
        "    x = self.flatten(x)\n",
        "    x = self.fc(x)\n",
        "    x = self.classifier(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "sNj0t_V1CK53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training\n",
        "model = MyVGG(num_classes=2)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "PG5uJz9bDnWB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}